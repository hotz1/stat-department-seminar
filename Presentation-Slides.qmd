---
title: 
  "<span style='color:#522a00; font-size:60pt'>
  The *Rocky Road* Towards Vanilla Bayesian Optimization in High Dimensions
  </span>"
title-slide-attributes: 
  data-background-color: "#f3e5ab40"
  data-background-image: img/rocky-road.webp
  data-background-size: 20%
  data-background-repeat: no-repeat
  data-background-position: bottom 15% right 10%
author: 
  "<span style='color:#522a00; font-size:30pt'>
  <br><br>
  Presenter: Joey Hotz<br>
  Supervisor: Dr. Geoff Pleiss
  </span>"
format: 
  revealjs:
    width: 1280
    height: 720
    slide-number: true
    theme: [default, custom.scss]
    toc: true
    toc-depth: 2
    toc-title: "Table of Contents"
include-after-body:
  - text: |
      <style>
        .reveal .slide ul {
        margin-bottom: 0;
        }
      </style>
editor: 
  source
---

# Introduction

## Research Motivation 

- Bayesian optimization is a statistical method for designing sequential experiments to locate the global optimum of a black-box function in a given some search space.
- This procedure tends to struggle in higher-dimensional search spaces, leading researchers to develop specialized algorithms.
- [A 2024 paper](https://arxiv.org/abs/2402.02229) claims that the only adjustment needed for higher-dimensional problems is to alter the parameters of the prior distribution.
  * The experimental results of this paper are limited in scope, while the claimed result is highly general.

# Statistical Background 

## Bayesian Optimization
  
- Bayesian optimization ([BayesOpt]{style="color:#fc5a8d"}) is a statistical methodology for efficiently finding the *true* global maximum of a black-box function $\tilde{f}(x)$.
  * $\tilde{f}$ is a real-valued function which we can query data from, but this process may be expensive or difficult. 
  * The function is defined on some compact [input space]{style="color:#fc5a8d"} $\mathcal{X} \subset \mathbb{R}^{D}$.
- The black-box optimization problem which we want to solve is: $$x^{\star} = \text{argmax}_{x \in \mathcal{X}}\tilde{f}(x)$$

## Gaussian Processes

### Definition

- A [Gaussian Process]{style="color:#fc5a8d"} (GP) is a statistical model which extends the finite-dimensional normal distribution to real-valued functions. 
  * These distributions are commonly used as surrogate models in Bayesian optimization for calculating the likelihood of the function $\tilde{f}$.
- A GP is fully specified by two functions; a *mean function* $\mu_{0}(\cdot)$ and a *kernel function* $\mathbf{k}(\cdot, \cdot)$.
  * From a Bayesian statistics perspective, this implies a prior distribution $\tilde{f}(x) \sim \mathcal{N}\left(\mu_{0}(x), \mathbf{k}(x, x)\right)$ for every $x \in \mathcal{X}$.

## Gaussian Processes {.unnumbered .unlisted}

### Illustrative Example

- As more sample observations $(x_{i}, y_{i})$ are collected, Gaussian process regression helps us refine the posterior predictive distributions for the function $\tilde{f}$.

:::: {.fragment .fade-in fragment-index=1}
![](img/GP-example-plots/blackbox-function-plot.png){.absolute left=0 top=300 width=47.5%}
::::

:::{.r-stack}

:::: {.fragment .fade-in fragment-index=1}
:::: {.fragment .fade-out fragment-index=2}
![](img/GP-example-plots/GP-prior.png){.absolute right=0 top=300 width=47.5%}
::::
::::

:::: {.fragment .fade-in fragment-index=2}
:::: {.fragment .fade-out fragment-index=3}
![](img/GP-example-plots/GP-posterior-1.png){.absolute right=0 top=300 width=47.5%}
::::
::::

:::: {.fragment .fade-in fragment-index=3}
:::: {.fragment .fade-out fragment-index=4}
![](img/GP-example-plots/GP-posterior-2.png){.absolute right=0 top=300 width=47.5%}
::::
::::

:::: {.fragment .fade-in fragment-index=4}
:::: {.fragment .fade-out fragment-index=5}
![](img/GP-example-plots/GP-posterior-3.png){.absolute right=0 top=300 width=47.5%}
::::
::::

:::: {.fragment .fade-in fragment-index=5}
:::: {.fragment .fade-out fragment-index=6}
![](img/GP-example-plots/GP-posterior-4.png){.absolute right=0 top=300 width=47.5%}
::::
::::

:::: {.fragment .fade-in fragment-index=6}
![](img/GP-example-plots/GP-posterior-5.png){.absolute right=0 top=300 width=47.5%}
::::

:::

## Gaussian Processes {.unnumbered .unlisted}

### Kernel Functions

:::: {.columns}

::: {.column width="75%"}
- The kernel $\mathbf{k}(\cdot, \cdot)$ of a GP specifies the covariance between outputs as a function of their respective arguments.
  * For example, if $g \sim \mathcal{GP}\left(\mu_{0}(\cdot), \mathbf{k}(\cdot, \cdot)\right)$, then $\mathbf{k}(x, x') = \textrm{Cov}\left(g(x), g(x')\right)$.
  * The kernel function is crucial in ensuring that the GP surrogate model accurately represents this relationship.
- A common class of kernels used in Bayesian optimization are [distance-based kernels]{style="color:#fc5a8d"}, in which $\mathbf{k}(x, x')$ is a function of the distance between $x$ and $x'$. 
:::
::: {.column width="25%"}
![](img/GP-example-plots/GP-diff-kernels-tall.png){.absolute right=0 bottom=0 height=90%}
:::
::::

## Gaussian Processes {.unnumbered .unlisted}

### Hyperparameters

- Typically, the kernel function $\mathbf{k}(\cdot, \cdot)$ for the surrogate model is a member of a parametric family of functions.
  * The hyperparameters of the kernel function for the surrogate model are estimated and updated at each iteration of the Bayesian optimization algorithm.
  * The learnable hyperparameters of the kernel function may have their own respective prior distributions.
  
## Gaussian Processes {.unnumbered .unlisted}

### Hyperparameters

- Common hyperparameters for distance-based kernel functions include the following:
  * [Lengthscale]{style="color:#fc5a8d"} ($\ell$), a $D$-dimensional vector which scales the distance between points in the input space. The distance is given by $d(x, x') = \sqrt{\sum_{d=1}^{D}\left(\frac{x_{d} - x'_{d}}{\ell_{d}}\right)^2}$.
  * [Observation noise]{style="color:#fc5a8d"} ($\sigma_{\epsilon}$), which represents uncertainty in the observed data. In particular, we assume $y_{i} \sim \mathcal{N}\left(\tilde{f}(x_{i}), \sigma_{\epsilon}^{2}\right)$.
  * [Outputscale]{style="color:#fc5a8d"} ($\sigma_{f}$), which scales the covariances.

## Gaussian Processes {.unnumbered .unlisted}

### Hyperparameters

![](img/GP-example-plots/GP-diff-lenscales.png){.absolute bottom=0 left=0 height=60%}

![](img/GP-example-plots/GP-diff-noises.png){.absolute top=0 right=0 height=47.5%}

![](img/GP-example-plots/GP-diff-outputscales.png){.absolute bottom=0 right=0 height=47.5%}

## Acquisition Functions

- In order to sequentially design experiments with Bayesian optimization, there must be some mechanism which determines the best input to use in the next iteration.
- An [acquisition function]{style="color:#fc5a8d"} is a non-negative function $u(x \vert \mathcal{D}_{n})$ which represents the potential utility gained by querying $\tilde{f}(x)$, based on the information provided by the dataset $\mathcal{D}_{n}$. 
- In a Bayesian optimization loop, $x_{n+1} = \text{argmax}_{x \in \mathcal{X}}u(x \vert \mathcal{D}_{n})$. 
- Two common acquisition functions are the Expected Improvement (EI) and Upper Confidence Bound (UCB).

# Previous Research

## The Curse of Dimensionality

- Though Bayesian optimization is commonly applied in many scientific research areas, it is known to struggle when optimizing over high-dimensional search spaces. 
- When utilizing a distance-based kernel function, high-dimensional Bayesian optimization algorithms often encounter the "*boundary issue*". 
  * In this phenomenon, the points which are being selected to be queried are on the boundary of the search space $\mathcal{X}$, as far as possible from any observed data. 
  * Without any considerable correlations between observed data, Bayesian optimization is hardly any better than purely random sampling. 
- The boundary issue is exacerbated by the dimension of the search space, as the maximal distance between points in $[0,1]^{D}$ is $\sqrt{D}$. 

## The "Vanilla BayesOpt" Paper

- In 2024, a paper published by Hvarfner et al. claimed that the boundary issue can be circumvented by simply changing the prior distribution for the lengthscales of the model.
  * In this paper, the authors propose that using lognormal distribution for the lengthscales which scales with $\sqrt{D}$ entirely circumvents the boundary issue.
- Their paper only shows the claimed result for Bayesian optimization with the RBF kernel, where acquisition is guided by expected improvement.
- Additionally, the mathematical justification supporting this adjustment to the prior distribution makes unrealistic assumptions about the model.

# Methods + Results

## Simulation Study

- To assess the robustness of the claimed result, we executed a broader simulation study based on the authors' code.
- We ran 20 simulations of Bayesian optimization under every combination of the following settings [(added simulations are highlighted)]{style="background-color:#93c572"}:
  * **Benchmark datasets (2)**: mopta (124 dimensions) and svm (388 dimensions)
  * **Acquisition functions (2)**: [Upper Confidence Bound]{style="background-color:#93c572"} and Expected Improvement
  * **Kernel functions (4)**: [Matern(1/2)]{style="background-color:#93c572"}, [Matern(3/2)]{style="background-color:#93c572"}, [Matern(5/2)]{style="background-color:#93c572"}, RBF
  * **Parametric models (4)**: MLE (no priors), Gamma(3,6) prior, $D$-scaled prior, and $D$-scaled prior with outputscale
- The parameters for the UCB function were tuned separately for each dataset based on a large hyperparameter sweep.

## UCB Tuning {.unlisted .unnumbered}

### Dataset: mopta (124 dimensions)

- We searched for the best $(\alpha, \gamma)$ for UCB with $\beta_{t} = \alpha\log(\gamma{t})$. In total, we ran 585 simulations across 9 values of $\alpha$ and 13 values of $\gamma$. 
- For the mopta dataset, we selected $\alpha = 0.75, \gamma = 0.25$. 

![](img/UCB-tuning/mopta-1000/UCB-mean-bars.png){.absolute bottom=0 left=0 height=55%}

![](img/UCB-tuning/mopta-1000/UCB-minimum-bars.png){.absolute bottom=0 right=0 height=55%}

## UCB Tuning {.unlisted .unnumbered}

### Dataset: svm (388 dimensions)

- We searched for the best $(\alpha, \gamma)$ for UCB with $\beta_{t} = \alpha\log(\gamma{t})$. In total, we ran 585 simulations across 9 values of $\alpha$ and 13 values of $\gamma$. 
- For the svm dataset, we selected $\alpha = 0.75, \gamma = 0.10$. 

![](img/UCB-tuning/svm-1000/UCB-mean-bars.png){.absolute bottom=0 left=0 height=55%}

![](img/UCB-tuning/svm-1000/UCB-minimum-bars.png){.absolute bottom=0 right=0 height=55%}

## Main Results

### Dataset: mopta08 (124 dimensions)

:::{.r-stack}

![](img/Final-Results/mopta-1000/comparison-by-acq-function-filtered.png){.absolute bottom=0 left=0 width=60%}

![](img/Final-Results/mopta-1000/comparison-by-model-filtered.png){.absolute top=0 right=0 width=40%}

![](img/Final-Results/mopta-1000/comparison-by-kernel-type-filtered.png){.absolute bottom=0 right=0 width=40%}

:::

## Main Results {.unlisted .unnumbered}

### Dataset: svm (388 dimensions)

:::{.r-stack}

![](img/Final-Results/svm-1000/comparison-by-acq-function-filtered.png){.absolute bottom=0 left=0 width=60%}

![](img/Final-Results/svm-1000/comparison-by-model-filtered.png){.absolute top=0 right=0 width=40%}

![](img/Final-Results/svm-1000/comparison-by-kernel-type-filtered.png){.absolute bottom=0 right=0 width=40%}

:::

## Conclusions

- The primary result of this project is that the result claimed by Hvarfner et al. is robust to changes to both the acquisition function and the kernel function.
- This result has broad implications for many applications of Bayesian optimization, as this allows for a much simpler  framework for performing high-dimensional optimization without requiring any specialized algorithms. 

## Future Research Directions

- Potential directions for future research in this topic include the following:
  * Changing acquisition functions (e.g. Probability of Improvement, Entropy Search)
  * Using non-distance kernel functions (such as the polynomial kernel) with similar scaling of inputs based on the dimension of the search space.
- Additionally, it would be enlightening to have a more robust and thorough version of the mathematical explanation given in the prior paper.

## Acknowledgements {.unnumbered .unlisted}

- [Geoff Pleiss]{style="color:#fc5a8d"}, for being a tremendous supervisor to work with throughout the past year of my program.
- [Colin Doumont]{style="color:#fc5a8d"} and [Donney Fan]{style="color:#fc5a8d"} for their assistance with coding the simulation studies.
- [My friends and family]{style="color:#fc5a8d"} for their support throughout my Masters program.
- [The audience]{style="color:#fc5a8d"} for being here today to see my presentation.